{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "#@title Environment Install\n",
    "# !conda install swig -y\n",
    "# !pip uninstall gym[all] -y\n",
    "# !pip install gym[all]\n",
    "\n",
    "\n",
    "# !conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch -y\n",
    "# !pip install ffmpeg\n",
    "# !pip install imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Program Reporsity Clone\n",
    "\n",
    "# !mkdir RL_WS\n",
    "# !git clone https://github.com/HenryHenry-Space/RL_xFLY.git RL_WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Add self designed environment into gym environemnt.\n",
    "# !mv /content/RL_WS/gym_env_change/gym_envs/__init__.py /usr/local/lib/python3.7/dist-packages/gym/envs/\n",
    "# !mv /content/RL_WS/gym_env_change/gym_envs_mujoco/__init__.py /usr/local/lib/python3.7/dist-packages/gym/envs/mujoco/\n",
    "# !mv /content/RL_WS/gym_env_change/gym_envs_mujoco/inverted_pendulum_v5_down.py /usr/local/lib/python3.7/dist-packages/gym/envs/mujoco/\n",
    "# !mv /content/RL_WS/gym_env_change/gym_envs_mujoco_assets/inverted_pendulum_down.xml /usr/local/lib/python3.7/dist-packages/gym/envs/mujoco/assets/\n",
    "\n",
    "\n",
    "#@title Add self designed environment into gym environemnt(server_conda_bas).\n",
    "# !cp ~/RL_WS/RL_xFLY/gym_env_change/gym_envs/__init__.py ~/miniconda3/lib/python3.8/site-packages/gym/envs/\n",
    "# !cp ~/RL_WS/RL_xFLY/gym_env_change/gym_envs_mujoco/__init__.py ~/miniconda3/lib/python3.8/site-packages/gym/envs/mujoco/\n",
    "# !cp ~/RL_WS/RL_xFLY/gym_env_change/gym_envs_mujoco/inverted_pendulum_v5_down.py ~/miniconda3/lib/python3.8/site-packages/gym/envs/mujoco/\n",
    "# !cp ~/RL_WS/RL_xFLY/gym_env_change/gym_envs_mujoco_assets/inverted_pendulum_down.xml ~/miniconda3/lib/python3.8/site-packages/gym/envs/mujoco/assets/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title Install tools for this program\n",
    "# # !pip install pyvirtualdisplay\n",
    "# !pip install tqdm\n",
    "# !pip install neptune-client\n",
    "# !sudo apt-get install xvfb\n",
    "# !sudo apt-get install -y xvfb x11-utils\n",
    "# !pip install pyvirtualdisplay==0.2.*\n",
    "# !sudo apt-get install libfdk-aac-dev libass-dev libopus-dev  \\\n",
    "# libtheora-dev libvorbis-dev libvpx-dev libssl-dev -y\n",
    "# !sudo apt-get install nasm\n",
    "# !sudo apt-get install libavcodec-extra\n",
    "# !pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "\n",
    "# path = \"../\"\n",
    "# # sys.path.append('set PATH=C:/Users/c3296143/.mujoco/mujoco200/bin;%PATH%')\n",
    "# # sys.path.append('set PATH=C://Users//c3296143//.mujoco//mujoco200//bin;%PATH%')\n",
    "# # sys.path.append('C:/Users/c3296143/.mujoco/mujoco200/bin')\n",
    "# # sys.path.append('C://Users//c3296143//.mujoco//mujoco200//bin')\n",
    "# # os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + 'C:/Users/c3296143/root/.mujoco/mujoco200/bin'\n",
    "# # old = os.environ.get(\"LD_LIBRARY_PATH\")\n",
    "# old = os.environ.get(\"PATH\")\n",
    "# # os.environ[\"PATH\"] = 'C:\\> set PATH=%PATH%;C:/Users/c3296143/.mujoco/mujoco200/bin'\n",
    "# # os.environ[\"PATH\"] = 'set PATH=C:/Users/c3296143/.mujoco/mujoco200/bin;%PATH%'\n",
    "\n",
    "# if old:\n",
    "#     os.environ[\"PATH\"] = old + \";\" +'C:\\\\Users\\\\c3296143\\\\.mujoco\\\\mujoco200\\\\bin'\n",
    "# #     os.environ[\"LD_LIBRARY_PATH\"] = old + \":\" + 'C:/Users/c3296143/.mujoco/mujoco200/bin'\n",
    "# # else:\n",
    "# #     os.environ[\"LD_LIBRARY_PATH\"] = 'C:/Users/c3296143/.mujoco/mujoco200/bin'\n",
    "# else:\n",
    "#     os.environ[\"PATH\"] = 'C:\\\\Users\\\\c3296143\\\\.mujoco\\\\mujoco200\\\\bin'\n",
    "\n",
    "# print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=False, size=(1400, 900))\n",
    "_ = display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glfw\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhnfly/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/gym/envs/registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import neptune.new as neptune\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from pyvirtualdisplay import Display\n",
    "# Display().start()\n",
    "import matplotlib\n",
    "import matplotlib_inline\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "import csv\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_data = open('reward_data.csv', mode='w') \n",
    "fieldnames = ['epoch', 'training reward', 'test reward']\n",
    "reward_writer = csv.DictWriter(reward_data, fieldnames=fieldnames)\n",
    "reward_writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhnfly/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/bravado_core/model.py:888: DeprecationWarning: jsonschema.RefResolver.in_scope is deprecated and will be removed in a future release.\n",
      "  with spec.resolver.in_scope(additional_uri):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/xhnfirst/RA-DDPG-IP-test/e/RAD-220\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "nep_log  = neptune.init(\n",
    "    project=\"xhnfirst/RA-DDPG-IP-test\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NTg5MDI2OS01MTVmLTQ2YjUtODA1Yy02ZWQyNDgxZDcwN2UifQ==\",\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhnfly/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/gym/utils/env_checker.py:144: UserWarning: \u001b[33mWARN: Agent's minimum observation space value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/xhnfly/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/gym/utils/env_checker.py:148: UserWarning: \u001b[33mWARN: Agent's maxmimum observation space value is infinity. This is probably too high\u001b[0m\n",
      "  logger.warn(\n",
      "/home/xhnfly/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/gym/envs/registration.py:619: UserWarning: \u001b[33mWARN: Env check failed with the following message: The `done` signal must be a boolean\n",
      "You can set `disable_env_checker=True` to disable this check.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('InvertedPendulum-v5-down')\n",
    "frame = []\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device = ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = 4\n",
    "        act_dim = 1\n",
    "        act_limit = 1\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit).to(device)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation).to(device)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('obs', 'act', 'rew', 'next_obs', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    # \"optimizer\": \"SGD\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"hid\": 64,\n",
    "    \"l\": 3,\n",
    "    \"seed\": 0,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"steps_video\": 10000,\n",
    "    \"epochs\": 1000,\n",
    "    \"replay_size\": int(1e8),\n",
    "    \"gamma\": 0.98,\n",
    "    \"polyak\": 0.995,\n",
    "    \"pi_lr\": 1e-4,\n",
    "    \"q_lr\": 1e-4,\n",
    "    \"batch_size\": 1000,\n",
    "    \"start_steps\": 3000, \n",
    "    \"update_after\": 1500,\n",
    "    \"update_every\": 300,\n",
    "    \"act_noise\": 0.01,\n",
    "    \"num_test_episodes\": 5,\n",
    "    \"max_ep_len\": 100,\n",
    "    \"max_video_len\": 100,\n",
    "    \"save_model_len\": 20000,\n",
    "    \"obs_dim\": 4,\n",
    "    \"act_dim\": 1,\n",
    "    \"act_limit\": 1\n",
    "}\n",
    "\n",
    "ac_kwargs=dict(hidden_sizes=[params[\"hid\"]]*params[\"l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim =  4\n",
      "act_dim =  1\n",
      "act_limit =  1\n"
     ]
    }
   ],
   "source": [
    "nep_log[\"parameters\"] = params\n",
    "\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "np.random.seed(params[\"seed\"])\n",
    "\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "print('obs_dim = ', params[\"obs_dim\"] )\n",
    "print('act_dim = ', params[\"act_dim\"])\n",
    "print('act_limit = ', params[\"act_limit\"])\n",
    "\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(**ac_kwargs)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "memory = ReplayMemory(params[\"replay_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "    a = torch.cat(data.act).float()\n",
    "    r = torch.cat(data.rew).float()\n",
    "    o2 =torch.cat(data.next_obs).float()\n",
    "    d = torch.cat(data.done).float()\n",
    "    q = ac.q(o,a)\n",
    "\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + params[\"gamma\"] * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "    return loss_q\n",
    "\n",
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "\n",
    "    o = torch.cat(data.obs).float()\n",
    "\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "\n",
    "    return -q_pi.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_optimizer = RMSprop(ac.pi.parameters(), lr=params[\"pi_lr\"])\n",
    "q_optimizer = RMSprop(ac.q.parameters(), lr=params[\"q_lr\"])\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q = compute_loss_q(data)\n",
    "\n",
    "    loss_q.backward()\n",
    "\n",
    "    q_optimizer.step()\n",
    "\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(params[\"polyak\"])\n",
    "            p_targ.data.add_((1 - params[\"polyak\"]) * p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    # print('a = ', a)\n",
    "    a += noise_scale * torch.randn(params[\"act_dim\"]).to(device)\n",
    "    return torch.clip(a, -params[\"act_limit\"], params[\"act_limit\"])\n",
    "\n",
    "def test_agent(epoch):\n",
    "    test_main = 0\n",
    "    test_step = 0\n",
    "    for j in range(params[\"num_test_episodes\"]):\n",
    "        obs, d, test_ep_ret, test_ep_len = env.reset(), False, 0, 0\n",
    "        o = obs\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        while not(test_ep_len == params[\"max_ep_len\"]):\n",
    "            a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "            obs, r, d, _ = env.step(a_cpu[0])\n",
    "            o = obs\n",
    "            o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "\n",
    "            test_ep_ret += r\n",
    "            test_ep_len += 1\n",
    "        test_ep_main = test_ep_ret/test_ep_len\n",
    "        test_step +=1\n",
    "        test_main += test_ep_main\n",
    "    test_reward = test_main/test_step\n",
    "    print('test_rew_main = ', float(test_reward))\n",
    "    nep_log[\"test/reward\"].log(test_reward)\n",
    "    return test_reward\n",
    "\n",
    "\n",
    "\n",
    "# def video_agent(epoch):\n",
    "#     screen = env.render(mode='rgb_array')\n",
    "#     im = Image.fromarray(screen)\n",
    "#     images = [im]\n",
    "#     obs, d, test_ep_ret, test_ep_len = env.reset(), False, 0, 0\n",
    "#     o = obs\n",
    "#     o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "#     while not(test_ep_len == params[\"max_video_len\"]):\n",
    "#         a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "#         obs, r, d, _ = env.step(a_cpu[0])\n",
    "#         screen = env.render(mode='rgb_array')\n",
    "#         images.append(Image.fromarray(screen))\n",
    "#         o = obs\n",
    "#         o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "#         test_ep_len += 1\n",
    "#     # print(\"begin writing image\")\n",
    "#     now = datetime.now()\n",
    "#     current_time = str(now.isoformat())\n",
    "#     image_file = 'images/inverted-pendulum-v5-%s%d.gif'% (current_time.replace(\":\",\"-\"), epoch)\n",
    "#     images[1].save(image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from IPython.display import Video\n",
    "# Video(\"images/inverted-pendulum-v5-2022-06-12T23-59-39.6029968.mp4\")\n",
    "\n",
    "def video_agent(epoch):\n",
    "    now = datetime.now()\n",
    "    current_time = str(now.isoformat())\n",
    "    video_file = 'images/inverted-pendulum-v5-%s%d.mp4'% (current_time.replace(\":\",\"-\"), epoch)\n",
    "    video = VideoRecorder(env, video_file)\n",
    "\n",
    "    env.render()\n",
    "    video.capture_frame()\n",
    "\n",
    "    obs, d, test_ep_ret, test_ep_len = env.reset(), False, 0, 0\n",
    "    o = obs\n",
    "    o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "    while not(test_ep_len == params[\"max_video_len\"]):\n",
    "        a_cpu = get_action(o, 0).cpu().data.numpy()\n",
    "        obs, r, d, _ = env.step(a_cpu[0])\n",
    "        env.render()\n",
    "        video.capture_frame()\n",
    "        o = obs\n",
    "        o = torch.tensor([o], dtype=torch.float32, device=device)\n",
    "        test_ep_len += 1\n",
    "    # print(\"begin writing image\")\n",
    "\n",
    "    video.close()\n",
    "    env.close()\n",
    "    Video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28700/1045472225.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  o = torch.tensor([o], device=device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "# o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "o =obs\n",
    "# env.viewer.set_camera(camera_id=0)\n",
    "\n",
    "\n",
    "# Define neutral value\n",
    "neutral = np.zeros(7)\n",
    "\n",
    "# Keep track of done variable to know when to break loop\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "total_steps = params[\"steps_per_epoch\"] * params[\"epochs\"]\n",
    "start_time = time.time()\n",
    "\n",
    "o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "start_time_rec = datetime.now()\n",
    "r_true = 0\n",
    "total_main = 0\n",
    "ep_rew_main = 0\n",
    "reward_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_q's state_dict:\n",
      "q.0.weight \t torch.Size([64, 5])\n",
      "q.0.bias \t torch.Size([64])\n",
      "q.2.weight \t torch.Size([64, 64])\n",
      "q.2.bias \t torch.Size([64])\n",
      "q.4.weight \t torch.Size([64, 64])\n",
      "q.4.bias \t torch.Size([64])\n",
      "q.6.weight \t torch.Size([1, 64])\n",
      "q.6.bias \t torch.Size([1])\n",
      "Model_pi's state_dict:\n",
      "pi.0.weight \t torch.Size([64, 4])\n",
      "pi.0.bias \t torch.Size([64])\n",
      "pi.2.weight \t torch.Size([64, 64])\n",
      "pi.2.bias \t torch.Size([64])\n",
      "pi.4.weight \t torch.Size([64, 64])\n",
      "pi.4.bias \t torch.Size([64])\n",
      "pi.6.weight \t torch.Size([1, 64])\n",
      "pi.6.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "model = ac.q\n",
    "print(\"Model_q's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "model = ac.pi\n",
    "print(\"Model_pi's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 498/500000 [00:01<23:40, 351.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.019157903974689205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 534/500000 [00:02<1:08:11, 122.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.05599997443650996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 967/500000 [00:03<23:13, 358.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.011597470124424561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1047/500000 [00:03<36:02, 230.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.05439997458798829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1470/500000 [00:04<21:30, 386.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.01639256337923433\n",
      "test_rew_main =  -0.05559997489123414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1975/500000 [00:15<1:28:19, 93.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.021988641521081827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2045/500000 [00:15<1:12:23, 114.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.05670878167854402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2499/500000 [00:25<2:53:03, 47.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.0027248047576908527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2567/500000 [00:26<1:54:46, 72.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.01039371400923381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2971/500000 [00:31<56:26, 146.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.013381814180599094\n",
      "test_rew_main =  -0.0030020143289919885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3486/500000 [00:43<1:21:12, 101.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.00817994056808632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3552/500000 [00:43<1:09:55, 118.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.0021483392780320895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3991/500000 [00:54<3:10:51, 43.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  0.020367387475352598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4055/500000 [00:54<2:06:39, 65.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.021544249420230956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4479/500000 [01:00<43:58, 187.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  0.002180875823030376\n",
      "test_rew_main =  -0.0026935559055059903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4984/500000 [01:12<1:29:48, 91.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.008731698780332638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5039/500000 [01:12<1:21:31, 101.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  0.0007395531103826264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5483/500000 [01:23<3:34:10, 38.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.0036117604132190435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5546/500000 [01:23<2:11:16, 62.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.01135221388994161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5992/500000 [01:30<40:59, 200.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.012054285482290308\n",
      "test_rew_main =  -0.015147686758111106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 6492/500000 [01:41<1:32:47, 88.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  0.001053758986084543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 6548/500000 [01:42<1:19:55, 102.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.014778539280143494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 6982/500000 [01:53<3:41:12, 37.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.00611592872178266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7039/500000 [01:53<2:24:26, 56.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  7.752138607930449e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7472/500000 [02:00<48:31, 169.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  3.4970140699197574e-08\n",
      "test_rew_main =  1.0629165475117828e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7967/500000 [02:12<1:38:49, 82.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  1.6715899933394948e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8036/500000 [02:12<1:12:19, 113.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  6.981036278319586e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8483/500000 [02:23<3:19:14, 41.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  7.293053589649823e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8537/500000 [02:24<2:23:51, 56.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  1.7768183715294317e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8968/500000 [02:30<53:18, 153.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.002385835702014537\n",
      "test_rew_main =  -0.0019814555097190816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9471/500000 [02:42<1:33:45, 87.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rew_main =  -0.004177954873628654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9530/500000 [02:42<1:17:30, 105.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rew_main =  -0.0011155698663060857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9600/500000 [02:45<2:21:08, 57.91it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000016?line=73'>74</a>\u001b[0m         \u001b[39m# Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000016?line=74'>75</a>\u001b[0m         \u001b[39m# detailed explanation). This converts batch-array of Transitions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000016?line=75'>76</a>\u001b[0m         \u001b[39m# to Transition of batch-arrays.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000016?line=76'>77</a>\u001b[0m         batch \u001b[39m=\u001b[39m Transition(\u001b[39m*\u001b[39m\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mtransitions))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000016?line=77'>78</a>\u001b[0m         update(data\u001b[39m=\u001b[39;49mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000016?line=79'>80</a>\u001b[0m \u001b[39m# End of epoch handling\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000016?line=80'>81</a>\u001b[0m \u001b[39mif\u001b[39;00m (t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39msteps_per_epoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb Cell 14'\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000012?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(data):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000012?line=4'>5</a>\u001b[0m     \u001b[39m# First run one gradient descent step for Q.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000012?line=7'>8</a>\u001b[0m     q_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000012?line=8'>9</a>\u001b[0m     loss_q \u001b[39m=\u001b[39m compute_loss_q(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000012?line=10'>11</a>\u001b[0m     loss_q\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000012?line=12'>13</a>\u001b[0m     q_optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb Cell 13'\u001b[0m in \u001b[0;36mcompute_loss_q\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000011?line=6'>7</a>\u001b[0m o2 \u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat(data\u001b[39m.\u001b[39mnext_obs)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000011?line=7'>8</a>\u001b[0m d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(data\u001b[39m.\u001b[39mdone)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000011?line=8'>9</a>\u001b[0m q \u001b[39m=\u001b[39m ac\u001b[39m.\u001b[39;49mq(o,a)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000011?line=11'>12</a>\u001b[0m \u001b[39m# Bellman backup for Q function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000011?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb Cell 9'\u001b[0m in \u001b[0;36mMLPQFunction.forward\u001b[0;34m(self, obs, act)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000007?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, obs, act):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000007?line=27'>28</a>\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq(torch\u001b[39m.\u001b[39;49mcat([obs, act], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xhnfly/RL_X/RL_xfly/algorithm/pytorch/DDPG/DDPG.ipynb#ch0000007?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msqueeze(q, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/RA_env_py38/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main loop: collect experience in env and update/log each epoch\n",
    "low = -1\n",
    "high = 1\n",
    "env.reset()\n",
    "for t in tqdm(range(total_steps)):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > params[\"start_steps\"]:\n",
    "        a = get_action(o, params[\"act_noise\"])      # Tensor\n",
    "        # print(\"a = \", a)\n",
    "        a_out = a.cpu().data.numpy()\n",
    "        a_cpu = a_out[0]\n",
    "    else:\n",
    "        a = torch.tensor([np.random.uniform(low, high)], dtype=torch.float32, device=device)\n",
    "        a_cpu = a.cpu().data.numpy()\n",
    "        # print(\"a = \", a)\n",
    "\n",
    "\n",
    "    # print(\"a_cpu = \", a_cpu[0])\n",
    "    # Step the env\n",
    "    obs2, r, d, _= env.step(a_cpu)\n",
    "    # print(\"env.step(a_cpu) = \", env.step(a_cpu))\n",
    "    # print(\"obs2 = \", obs2)\n",
    "\n",
    "\n",
    "    o2 = obs2\n",
    "\n",
    "\n",
    " \n",
    "    ep_len += 1\n",
    "    total_main += r\n",
    "    ep_ret += r\n",
    "\n",
    "    # print(\"a = \", a)\n",
    "    a_s = torch.tensor([a_cpu], dtype=torch.float32, device=device)\n",
    "    # print(\"a_s = \", a_s)\n",
    "    o2 = torch.tensor([o2], dtype=torch.float32, device=device)\n",
    "    r = torch.tensor([r], dtype=torch.float32, device=device)\n",
    "    d = torch.tensor([d], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    memory.push(o, a_s, r, o2, d)\n",
    "    # print(\"o = \", o)\n",
    "    # print(\"a = \", a)\n",
    "    nep_log[\"train/o\"].log(o)\n",
    "    nep_log[\"train/a\"].log(a)\n",
    "    nep_log[\"train/r\"].log(r)\n",
    "    nep_log[\"train/o2\"].log(o2)\n",
    "    nep_log[\"train/d\"].log(d)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o=o2\n",
    "\n",
    "    \n",
    "    \n",
    "    # End of trajectory handling\n",
    "    if (ep_len == params[\"max_ep_len\"]):\n",
    "        ep_rew = ep_ret/params[\"max_ep_len\"]\n",
    "        ep_rew_main += ep_rew\n",
    "        obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        # o = list(obs['robot0_proprio-state']) + list(obs['object-state'])\n",
    "        o = obs\n",
    "        o = torch.tensor([o], device=device)\n",
    "\n",
    "\n",
    "    # Update handling\n",
    "    if t >= params[\"update_after\"] and t % params[\"update_every\"] == 0:\n",
    "        for i in range(params[\"update_every\"]):\n",
    "\n",
    "            transitions = memory.sample(params[\"batch_size\"])\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % params[\"steps_per_epoch\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        train_reward = ep_rew_main/(params[\"steps_per_epoch\"]/params[\"max_ep_len\"])\n",
    "        nep_log[\"train/reward\"].log(train_reward)\n",
    "        nep_log[\"train/total_main\"].log(total_main)\n",
    "        # print('train_rew_main = ', train_reward.cpu().data.numpy()[0])\n",
    "        print('train_rew_main = ', train_reward)\n",
    "        ep_rew_main = 0\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_reward = test_agent(epoch)\n",
    "        reward_writer.writerow({'epoch': epoch, 'training reward': train_reward, 'test reward': test_reward})\n",
    "        \n",
    "\n",
    "    if (t+1) % params[\"steps_video\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        # now = datetime.now()\n",
    "        # current_time = str(now.isoformat())\n",
    "        # print('current_time = ', current_time)\n",
    "        video_agent(epoch)\n",
    "        # now = datetime.now()\n",
    "        # current_time = str(now.isoformat())\n",
    "        # print('current_time = ', current_time)\n",
    "\n",
    "    if (t+1) % params[\"save_model_len\"] == 0:\n",
    "        epoch = (t+1) // params[\"steps_per_epoch\"]\n",
    "        now = datetime.now()\n",
    "        current_time = str(now.isoformat())\n",
    "        torch.save({\n",
    "                    'model of ac.q': ac.q.state_dict(),\n",
    "                    'model of ac.pi': ac.pi.state_dict(),\n",
    "                    'q_optimizer_state_dict': q_optimizer.state_dict(),\n",
    "                    'pi_optimizer_state_dict': pi_optimizer.state_dict(),\n",
    "\n",
    "                    }, \"model_nn/model_nn_%s%d.pt\" % (current_time.replace(\":\",\"-\"), epoch))\n",
    "reward_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pi_optimizer's state_dict:\")\n",
    "for var_name in pi_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", pi_optimizer.state_dict()[var_name])\n",
    "\n",
    "print(\"q_optimizer's state_dict:\")\n",
    "for var_name in q_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", q_optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = str(now.isoformat())\n",
    "\n",
    "\n",
    "\n",
    "torch.save({\n",
    "            'model of ac.q': ac.q.state_dict(),\n",
    "            'model of ac.pi': ac.pi.state_dict(),\n",
    "            'q_optimizer_state_dict': q_optimizer.state_dict(),\n",
    "            'pi_optimizer_state_dict': pi_optimizer.state_dict(),\n",
    "\n",
    "            }, \"model_nn/model_nn_%s%d.pt\" % (current_time.replace(\":\",\"-\"), epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_log.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"images/inverted-pendulum-v5-2022-06-12T23-59-39.6029968.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a58f15cac8bdca84d481529dff1da9ea38dcd1e42654bb267244e94160251abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RA_env_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
